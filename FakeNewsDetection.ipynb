{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a06c4d1-9de1-45bc-9984-67a00c0f6a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: streamlit in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (1.41.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: psutil in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from accelerate) (2.5.1+cu118)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.21.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy transformers scikit-learn streamlit datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd1eedc-4c2f-4965-82eb-3bd8e20cafad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (0.20.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e78711a-52ea-4945-aac2-4989018b492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c041a92a-308b-4224-9116-8ea6f4710be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required resources for NLP processing\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2303c13-3c9f-47b7-aa24-ddaf3c8a0cb3",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebf5bf73-9a95-4d99-8488-def975492404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.tsv', sep='\\t', header=None) # reads the .csv files\n",
    "valid_data = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "\n",
    "columns = [\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
    "           \"Barely True Count\", \"False Count\", \"Half True Count\", \"Mostly True Count\",\n",
    "           \"Pants on Fire Count\", \"Context\"]\n",
    "train_data.columns = valid_data.columns = test_data.columns = columns # defines the columns that are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d4b88d3-6569-461a-a26e-23ff3f19bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selects onky the statement and label colums from the datasets and removes all others, \n",
    "#this helps filter out unnessary coloumns \n",
    "train_data = train_data[[\"Statement\", \"Label\"]]\n",
    "valid_data = valid_data[[\"Statement\", \"Label\"]]\n",
    "test_data = test_data[[\"Statement\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b28d0d63-cdcd-4971-b671-356c62594f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the lemmatizer and gets stopword list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3edec09a-8c66-4cb4-9a09-a0de1b23bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenizes the sentences\n",
    "    words = word_tokenize(text.lower())  # converts to lowercase and tokenize\n",
    "    \n",
    "    filtered_tokens = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Removes stopwords and applys lemmatization\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Rejoins words into a cleaned sentence\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d822db8-6a26-4dee-9193-76d2bba83d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessing to the datasets\n",
    "train_data[\"Statement\"] = train_data[\"Statement\"].apply(preprocess_text)\n",
    "valid_data[\"Statement\"] = valid_data[\"Statement\"].apply(preprocess_text)\n",
    "test_data[\"Statement\"] = test_data[\"Statement\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "658ccc3f-9103-4813-bc30-38999a4be77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(label): # only choosing the label argument to convert categorical labels to numerical values\n",
    "    return 0 if label in ['false', 'barely-true', 'pants-fire'] else 1 # returns 0 if the label is false otherwise return 1\n",
    "\n",
    "train_data['Label'] = train_data['Label'].apply(map_labels)\n",
    "valid_data['Label'] = valid_data['Label'].apply(map_labels)\n",
    "test_data['Label'] = test_data['Label'].apply(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ddfd1d4-6ab2-41d9-9a75-fcbbcde39248",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features \n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['Statement'])\n",
    "X_valid_tfidf = vectorizer.transform(valid_data['Statement'])\n",
    "X_test_tfidf = vectorizer.transform(test_data['Statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0be140a-d980-4484-8d32-f0f3db7733fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts the the label coloum and adds that to the y_train y_valid and y_test \n",
    "y_train = train_data['Label']\n",
    "y_valid = valid_data['Label']\n",
    "y_test = test_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3497c75-abf9-496e-beb1-6670a03198ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ee3fb6b-8821-4243-b9fb-7be0a72d0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_model(model, X_train, X_valid, y_train, y_valid, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_valid, y_pred, average='weighted')\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy * 100,\n",
    "        \"Precision\": precision * 100,\n",
    "        \"Recall\": recall * 100,\n",
    "        \"F1 Score\": f1 * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4dbf0e-0a05-4ce6-8a70-1daf930e0433",
   "metadata": {},
   "source": [
    "## Implementing traditional Models (Logistic Regression, SVM, Random Forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7a6fd35-fd95-47d5-a92a-e052964c0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea7fbdb8-5a28-41ec-abea-2025afe39d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000) # this ensures that it goes through enough iterations\n",
    "ml_results.append(evaluate_ml_model(lr_model,\n",
    "                                    X_train_tfidf,\n",
    "                                    X_valid_tfidf,\n",
    "                                    y_train,\n",
    "                                    y_valid,\n",
    "                                    \"Logistic Regression\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8ea6eed-1c0e-408f-bbd4-abdd1c776a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing SVC from the sklearn \n",
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0d2576b-d7d1-485e-acb9-8a3c91bc1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_model = SVC() #creating a svm moddel from sklearn thats helpful in text classification\n",
    "ml_results.append(evaluate_ml_model(svm_model,\n",
    "                                    X_train_tfidf,\n",
    "                                    X_valid_tfidf,\n",
    "                                    y_train,\n",
    "                                    y_valid,\n",
    "                                    \"SVM\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bba9731-0999-4f19-8ff8-2bab58b3ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100) #this creates random forest classifier \n",
    "#with 100 decision trees\n",
    "ml_results.append(evaluate_ml_model(rf_model,\n",
    "                                    X_train_tfidf,\n",
    "                                    X_valid_tfidf,\n",
    "                                    y_train,\n",
    "                                    y_valid,\n",
    "                                    \"Random Forest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a272ff-d3fa-4bf5-b674-c16e7d5bef3a",
   "metadata": {},
   "source": [
    "## Implementing and fine-tunning distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d99a1240-ebd1-4b79-a630-57fe47bb0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the pre-trained distilBERT model\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') \n",
    "#the uncased means that its not case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cd31501-ece6-47fd-ab28-00e1e41ec434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(data, tokenizer, max_length=128): \n",
    "    return tokenizer(data.tolist(),\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")\n",
    "    \n",
    "# adding padding and truncation, making sure that all sequences are same length\n",
    "#trucation set to cut off text that is longer than 128 tokens \n",
    "# return_tensor=\"pt\" is used to make sure that the output is in pytorch format whihc is a requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ec923ff-0ba9-4d94-ade7-54251f4dd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the encode function to all the datasets \n",
    "\n",
    "train_encodings = encode_texts(train_data['Statement'], distilbert_tokenizer)\n",
    "valid_encodings = encode_texts(valid_data['Statement'], distilbert_tokenizer)\n",
    "test_encodings = encode_texts(test_data['Statement'], distilbert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "325f032c-3571-4406-86f4-a27958f2cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing torch to make sure that its up\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "#importing modules are are needed\n",
    "#to handle datasets and batch loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc6b5392-4557-47b0-84cf-ea56de7f073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating TensorDatasets for training, validations, and testing\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'],\n",
    "                              train_encodings['attention_mask'],\n",
    "                              torch.tensor(train_data['Label'].tolist()))\n",
    "\n",
    "valid_dataset = TensorDataset(valid_encodings['input_ids'],\n",
    "                              valid_encodings['attention_mask'],\n",
    "                              torch.tensor(valid_data['Label'].tolist()))\n",
    "\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'],\n",
    "                             test_encodings['attention_mask'],\n",
    "                             torch.tensor(test_data['Label'].tolist()))\n",
    "\n",
    "#attention_mask indicated which token is actual word(1) and which is not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "321dd1f6-4c8a-41e2-9956-e622f5ef07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating DataLoaders for training, validations and testing \n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) \n",
    "# shuffle is used to ensure that the data is shuffled in each epouch for different results\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a090971-cc81-4491-a6b8-7822a8e6d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Loading the distilBERT model \n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = model.to('cuda') #using GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa3b605b-8505-4a71-acd4-ca462f14caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01) #setting the learning rate of AdamW to be 5e-5\n",
    "loss_fn = torch.nn.CrossEntropyLoss() #defineing the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f289942b-1c68-4996-9db2-9f28a65dcc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec1bb63c-ee54-4396-89c8-8eb0fb8006b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Epoch 6 completed.\n",
      "Epoch 7 completed.\n",
      "Epoch 8 completed.\n",
      "Epoch 9 completed.\n",
      "Epoch 10 completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    model.train() # seting the model into training mode\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [x.to('cuda') for x in batch] # moves the data to the GPU\n",
    "        optimizer.zero_grad() # clears the old weights if any \n",
    "        outputs = model(input_ids, attention_mask=attention_mask) # passes the input data through the model\n",
    "        loss = loss_fn(outputs.logits, labels) # shows how accurate the predictions were from the actual labels \n",
    "        loss.backward() # learn from the former predictions and adjusts \n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "154269af-c3e6-4f59-a58b-08843a3a2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluating the models \n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [x.to('cuda') for x in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# the metrics \n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "ml_results.append({\n",
    "    \"Model\": \"DistilBERT\",\n",
    "    \"Accuracy\": accuracy * 100,\n",
    "    \"Precision\": precision * 100,\n",
    "    \"Recall\": recall * 100,\n",
    "    \"F1 Score\": f1 * 100\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "febd1274-4c43-405c-8352-24f1ef976c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model   Accuracy  Precision     Recall   F1 Score\n",
      "0  Logistic Regression  61.760125  62.084163  61.760125  61.013597\n",
      "1                  SVM  62.538941  63.425728  62.538941  61.314431\n",
      "2        Random Forest  60.280374  60.678494  60.280374  59.240332\n",
      "3           DistilBERT  60.220994  60.602454  60.220994  60.345947\n"
     ]
    }
   ],
   "source": [
    "#Displaying results in a tabular form \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "df_results = pd.DataFrame(ml_results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f816645-78cd-4d0b-9911-0e3f0a80b9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
