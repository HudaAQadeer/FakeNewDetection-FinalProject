{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a06c4d1-9de1-45bc-9984-67a00c0f6a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: streamlit in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (1.41.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.21.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy transformers scikit-learn streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd1eedc-4c2f-4965-82eb-3bd8e20cafad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (0.20.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hudas\\finalproject\\fnvenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e78711a-52ea-4945-aac2-4989018b492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c041a92a-308b-4224-9116-8ea6f4710be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hudas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required resources for NLP processing\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2303c13-3c9f-47b7-aa24-ddaf3c8a0cb3",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf5bf73-9a95-4d99-8488-def975492404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.tsv', sep='\\t', header=None) # reads the .csv files\n",
    "valid_data = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "\n",
    "columns = [\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
    "           \"Barely True Count\", \"False Count\", \"Half True Count\", \"Mostly True Count\",\n",
    "           \"Pants on Fire Count\", \"Context\"]\n",
    "train_data.columns = valid_data.columns = test_data.columns = columns # defines the columns that are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4b88d3-6569-461a-a26e-23ff3f19bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selects onky the statement and label colums from the datasets and removes all others, \n",
    "#this helps filter out unnessary coloumns \n",
    "train_data = train_data[[\"Statement\", \"Label\"]]\n",
    "valid_data = valid_data[[\"Statement\", \"Label\"]]\n",
    "test_data = test_data[[\"Statement\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d0d63-cdcd-4971-b671-356c62594f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the lemmatizer and gets stopword list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edec09a-8c66-4cb4-9a09-a0de1b23bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenizes the sentences\n",
    "    words = word_tokenize(text.lower())  # converts to lowercase and tokenize\n",
    "    \n",
    "    filtered_tokens = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Removes stopwords and applys lemmatization\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Rejoins words into a cleaned sentence\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d822db8-6a26-4dee-9193-76d2bba83d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessing to the datasets\n",
    "train_data[\"Statement\"] = train_data[\"Statement\"].apply(preprocess_text)\n",
    "valid_data[\"Statement\"] = valid_data[\"Statement\"].apply(preprocess_text)\n",
    "test_data[\"Statement\"] = test_data[\"Statement\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ccc3f-9103-4813-bc30-38999a4be77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(label): # only choosing the label argument to convert categorical labels to numerical values\n",
    "    return 0 if label in ['false', 'barely-true', 'pants-fire'] else 1 # returns 0 if the label is false otherwise return 1\n",
    "\n",
    "train_data['Label'] = train_data['Label'].apply(map_labels)\n",
    "valid_data['Label'] = valid_data['Label'].apply(map_labels)\n",
    "test_data['Label'] = test_data['Label'].apply(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfd1d4-6ab2-41d9-9a75-fcbbcde39248",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features \n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['Statement'])\n",
    "X_valid_tfidf = vectorizer.transform(valid_data['Statement'])\n",
    "X_test_tfidf = vectorizer.transform(test_data['Statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be140a-d980-4484-8d32-f0f3db7733fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts the the label coloum and adds that to the y_train y_valid and y_test \n",
    "y_train = train_data['Label']\n",
    "y_valid = valid_data['Label']\n",
    "y_test = test_data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a36314-3f1f-4441-ae87-aa69f19d0172",
   "metadata": {},
   "source": [
    "## Implementing traditional Models (Logistic Regression, SVM, Random Forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6fd35-fd95-47d5-a92a-e052964c0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_model(model, X_train, X_valid, y_train, y_valid):\n",
    "    model.fit(X_train, y_train) #this trains the model on the training data\n",
    "    y_pred = model.predict(X_valid) # predicts labels for the validation datasets\n",
    "    print(classification_report(y_valid, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fbdb8-5a28-41ec-abea-2025afe39d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000) # this ensures that it goes through enough iterations\n",
    "evaluate_ml_model(lr_model, X_train_tfidf, X_valid_tfidf, y_train, y_valid) # the model is evaluated using this function\n",
    "\n",
    "print(\"Logistic Regression Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea6eed-1c0e-408f-bbd4-abdd1c776a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing SVC from the sklearn \n",
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4638a5-83db-4aa5-9191-663c0b31ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_model = SVC() #creating a svm moddel from sklearn thats helpful in text classification\n",
    "evaluate_ml_model(svm_model, X_train_tfidf, X_valid_tfidf, y_train, y_valid) #the X_train_tfidf and X_valid_tfidf help\n",
    "#with converting text numerically\n",
    "\n",
    "print(\"SVM Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba9731-0999-4f19-8ff8-2bab58b3ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100) #this creates random forest classifier \n",
    "#with 100 decision trees\n",
    "evaluate_ml_model(rf_model, X_train_tfidf, X_valid_tfidf, y_train, y_valid)\n",
    "\n",
    "print(\"Random Forest Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d519ab-84eb-45d7-90c6-93e28c2507ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementing and fine-tunning DistilBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a1240-ebd1-4b79-a630-57fe47bb0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the pre-trained distilBERT model\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') \n",
    "#the uncased means that its not case sensetive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd31501-ece6-47fd-ab28-00e1e41ec434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(data, tokenizer, max_length=128): \n",
    "    return tokenizer(data.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "# adding padding and truncation, making sure that all sequences are same length\n",
    "#trucation set to cut off text that is longer than 128 tokens \n",
    "# return_tensor=\"pt\" is used to make sure that the output is in pytorch format whihc is a requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec923ff-0ba9-4d94-ade7-54251f4dd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the encode function to all the datasets \n",
    "\n",
    "train_encodings = encode_texts(train_data['Statement'], tokenizer)\n",
    "valid_encodings = encode_texts(valid_data['Statement'], tokenizer)\n",
    "test_encodings = encode_texts(test_data['Statement'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f032c-3571-4406-86f4-a27958f2cc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b5392-4557-47b0-84cf-ea56de7f073e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dd1f6-4c8a-41e2-9956-e622f5ef07a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a090971-cc81-4491-a6b8-7822a8e6d1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
